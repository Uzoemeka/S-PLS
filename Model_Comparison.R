# devtools :: install _ github ( ’ s elb ou ha dd an i / PPLS / Package /PPLS ’)
### Load Required Libraries
#________________________________________________________________________________ 
#__________________________ data simulation _____________________________________
#________________________________________________________________________________ 
library(mvtnorm)
library(PPLS)
library(pracma)
library(Renvlp)
library(pls)
library(matrixStats)
library(fBasics)
library(Hmisc)
library(ggplot2)
library(reshape2)
library(simrel)
library(future.apply)
library(MASS)
library(OmicsPLS)
library(PO2PLS)

### Initialize Environment
rm(list = ls())
set.seed(100)

### Define Simulation Parameters
runs <- 100  # Number of replications
n <- 30      # Sample size
p <- 5       # Number of predictors
r <- 3       # Number of response variables
q <- 2       # Number of latent variables
delt <- 0.8  # Correlation decay parameter
rho <- 0.01  # Covariance decay parameter

### Define Gram-Schmidt Process for Orthonormalization
gram_schmidt <- function(x) {
  x <- as.matrix(x)
  m <- nrow(x)
  n <- ncol(x)
  q <- matrix(0, m, n)
  r <- matrix(0, n, n)
  
  process_column <- function(j) {
    v <- x[, j]
    
    if (j > 1) {
      lapply(1:(j - 1), function(i) {
        r[i, j] <<- t(q[, i]) %*% x[, j]
        v <<- v - r[i, j] * q[, i]
      })
    }
    
    r[j, j] <<- sqrt(sum(v^2))
    q[, j] <<- v / r[j, j]
  }
  
  lapply(1:n, process_column)
  
  list(Q = q, R = r)
}


### Generate Gamma Matrix
gamma <- gram_schmidt(matrix(rnorm(p * p, 0, 1), p, p))$Q
gamma1 <- gamma[, 1:q]
gamma0 <- gamma[, (q + 1):p]

### Construct Covariance Matrix sig
sig <- outer(1:p, 1:p, function(i, j) rho^(abs(i - j)))

### Compute Eigenvalues and Construct sigx
omega <- sort(eigen(sig)$values, decreasing = FALSE)
sigx <- gamma1 %*% diag(omega[1:q], q) %*% t(gamma1) +
  gamma0 %*% diag(omega[(q + 1):p], (p - q)) %*% t(gamma0)

### Construct sigy.x Matrix
sigy.x <- outer(1:r, 1:r, function(i, j) delt^(abs(i - j)))


### Generate Beta Matrix
Beta <- gamma1 %*% matrix(runif(q * r, 0, 2), q, r)

### Simulate Data
# Create lists of the required lengths
X <- vector("list", runs)
E <- vector("list", runs)
Y <- vector("list", runs)

# Use lapply to replace the for loop
lapply(1:runs, function(k) {
  X[[k]] <<- scale(mvrnorm(n, rep(0, p), sigx))  # scale after generating X
  E[[k]] <<- mvrnorm(n, rep(0, r), sigy.x)  # E is generated
  Y[[k]] <<- scale(X[[k]] %*% Beta + E[[k]])  # Y is generated by the matrix operation
})




#________________________________________________________________________________ 
#___________________________________________ Using the Envelope
#________________________________________________________________________________ 

library(pls)
library(future)
library(future.apply)

# Plan for multiprocessing
plan(multisession)  # Alternatively, you can use multiprocess if running on local machines or multisession for remote execution

fold = 10
groups <- sample(rep(seq_len(fold), length.out = n))  # Groups for cross-validation

# Define the envelope cross-validation function
myenvCv <- function(X, Y, k) {
  Y <- as.matrix(Y)
  X <- as.matrix(X)
  
  # Dimensions of Y and X
  a <- dim(Y)
  n <- a[1]  # Number of observations
  r <- a[2]  # Number of responses
  p <- ncol(X)  # Number of predictors
  
  # PLS fitting and initial loadings
  fitt <- plsr(Y ~ X, ncomp = p, method = "simpls")$loadings
  M <- list()  # Empty list to store models
  
  # Matrix to store fitted models for all folds and components
  efit <- array(NA, dim = c(k, p))  # Replace list with array to hold fitted models
  
  # Generate the fitted models using envelope regression
  # We do this in matrix form by iterating over components and folds
  efit[] <- mapply(function(i, j) {
    xenv(X[groups != i, ], Y[groups != i, ], u = j, asy = FALSE, as.matrix(fitt[, 1:j]))
  }, rep(1:k, each = p), rep(1:p, times = k), SIMPLIFY = FALSE)
  
  # Calculate the prediction errors (predicted Y - actual Y) in matrix form
  prederror <- array(NA, dim = c(k, p))
  
  prederror[] <- mapply(function(i, j) {
    Y[groups == i, ] - X[groups == i, ] %*% efit[[i, j]]$beta
  }, rep(1:k, each = p), rep(1:p, times = k), SIMPLIFY = FALSE)
  
  # Compute the Frobenius norm for the cross-validation errors
  ecv <- matrix(0, k, p)
  
  ecv[] <- mapply(function(i, j) {
    norm(prederror[[i, j]], type = "F")  # Frobenius norm of the error
  }, rep(1:k, each = p), rep(1:p, times = k))
  
  # Return the mean cross-validation error for each component
  return(colMeans(ecv))
}

# Parallelize the cross-validation for each run
envrepp1 <- matrix(unlist(future_lapply(1:runs, function(i) myenvCv(X = X[[i]], Y = Y[[i]], k = fold))), 
                   ncol = p - 1, byrow = TRUE)

# Calculate mean and standard deviation of cross-validation errors
ecv_mn1 <- colMeans(envrepp1)
ecv_sd1 <- apply(envrepp1, 2, sd)

# Save results to file
sink("EIRn30p5r3q2delta0_8_rho0_01.txt")
print(envrepp1)
print(colMeans(envrepp1))
print(colSds(envrepp1))
sink()





#________________________________________________________________________________ 
#_______________________________________Using the SIMPLS ________________________
#________________________________________________________________________________ 

library(pls)
library(future)
library(future.apply)

# Plan for parallel processing
plan(multisession)  # Use multisession for parallel processing

# Define the function for cross-validation using PLS with simpls method
mysimplsCv <- function(X, Y, k) {
  # Ensure Y and X are matrices
  Y <- as.matrix(Y)
  X <- as.matrix(X)
  
  # Get dimensions
  a <- dim(Y)
  n <- a[1]  # Number of observations
  p <- ncol(X)  # Number of predictors
  
  # Preallocate list to store models
  sfit <- vector("list", k)  # List to store fitted models
  
  # Fit models for each fold
  sfit <- future_lapply(1:k, function(i) {
    plsr(Y[groups != i, ] ~ X[groups != i, ], ncomp = p, method = "simpls")
  })
  
  # Preallocate the matrix for cross-validation errors
  cv <- matrix(0, k, p)
  
  # Compute the cross-validation errors (norms)
  cv[] <- mapply(function(i, j) {
    norm(
      Y[groups == i, ] - X[groups == i, ] %*% sfit[[i]]$coefficients[, , j], 
      type = "F"
    )
  }, rep(1:k, each = p), rep(1:p, times = k))
  
  # Return the column means of the cross-validation errors
  return(colMeans(cv))
}

# Parallelize the cross-validation for each run
simplsrepp1 <- matrix(unlist(future_lapply(1:runs, function(i) {
  mysimplsCv(X = X[[i]], Y = Y[[i]], k = fold)
})), ncol = p, byrow = TRUE)

# Calculate mean and standard deviation of cross-validation errors
scv_mn1 <- colMeans(simplsrepp1)
scv_sd1 <- apply(simplsrepp1, 2, sd)

# Save results to file
sink("SIRn30p5r3q2delta0_8_rho0_01.txt")
print(simplsrepp1)
print(colMeans(simplsrepp1))
print(colSds(simplsrepp1))
sink()




#________________________________________________________________________________ 
#____________________________________ Using the EM - PLS _______________________ 
#________________________________________________________________________________ 


library(future)
library(future.apply)
library(MASS)  # Assuming ginv comes from MASS

# Plan for parallel execution
plan(multisession)  # Use multisession for parallel processing

myPPLSCv <- function(X, Y, k, emnum) {
  # Convert Y and X to matrices
  Y <- as.matrix(Y)
  X <- as.matrix(X)
  
  # Get dimensions
  a <- dim(Y)
  n <- a[1]  # Number of observations
  r <- a[2]  # Number of response variables
  p <- ncol(X)  # Number of predictors
  
  # Preallocate the list and matrix for fitted models
  # Preallocate the list for storing the fitted models
  pfit <- vector("list", k * r)
  
  # Use mapply to apply PO2PLS across both fold and response variable indices
  pfit[] <- mapply(function(i, j) {
    PO2PLS(X[groups != i,], Y[groups != i,], r = j, 0, 0, steps = emnum)
  }, rep(1:k, each = r), rep(1:r, times = k))
  
  
  # Replace the nested for loops with mapply
  pp.pred <- matrix(0, k, r)  # Preallocate the matrix
  
  # Use mapply to iterate over i (fold) and j (response variable)
  pp.pred[] <- mapply(function(i, j) {
    # Extract the fitted model
    model <- pfit[[ (i - 1) * r + j ]]
    
    # Calculate the prediction error using Frobenius norm
    norm(
      as.matrix(
        Y[groups == i,] - X[groups == i,] %*% ginv(model$params$W) %*% model$params$SigT %*% t(model$params$W) %*% 
          model$params$W %*% model$params$SigT %*% model$params$B %*% t(model$params$C)
      ), 
      type = "F"
    )
  }, rep(1:k, each = r), rep(1:r, times = k))  # Repeat indices for mapply
  
  # Now pp.pred is filled with the Frobenius norms
  
  
  # Return column-wise mean of the prediction errors
  return(colMeans(pp.pred))
}

# Parallelize the cross-validation for each run
PPLSrepp1 <- matrix(unlist(future_lapply(1:runs, function(i) {
  myPPLSCv(X = X[[i]], Y = Y[[i]], k = fold, emnum = 150)
})), ncol = r, byrow = TRUE)

# Calculate mean and standard deviation of cross-validation errors
pcv_mn1 <- colMeans(PPLSrepp1)
pcv_sd1 <- apply(PPLSrepp1, 2, sd)

# Save results to file
sink("PIRn30p5r3q2delta0_8_rho0_01.txt")
print(PPLSrepp1)
print(colMeans(PPLSrepp1))
print(colSds(PPLSrepp1))
sink()



#________________________________________________________________________________ 
#__________________________________ Using the OLS ______________________________
#________________________________________________________________________________ 

library(future)
library(future.apply)

# Plan for parallel execution
plan(multisession)  # Use multisession for parallel processing

mlmCV <- function(X, Y, k) {
  Y <- as.matrix(Y)
  X <- as.matrix(X)
  
  # Get the dimensions
  a <- dim(Y)
  n <- a[1]  # Number of observations
  r <- a[2]  # Number of response variables
  p <- ncol(X)  # Number of predictors
  
  # Preallocate the list for models and predictions
  mlm1 <- vector("list", k)  # List to store models for each fold
  Yhat1 <- vector("list", k)  # List to store predictions for each fold
  
  # Fit models for each fold
  mlm1 <- future_lapply(1:k, function(i) {
    lm(cbind(Y[groups != i, ]) ~ X[groups != i, ])
  })
  
  # Predict for each fold
  Yhat1 <- future_lapply(1:k, function(i) {
    X[groups == i, ] %*% coef(mlm1[[i]])[-1, ]
  })
  
  # Calculate Frobenius norm for each fold
  mlmnorm <- future_lapply(1:k, function(i) {
    norm(as.matrix(Y[groups == i, ] - Yhat1[[i]]), type = "F")
  })
  
  # Return the mean of the norms for each response variable
  return(rep(mean(unlist(mlmnorm)), p))
}

# Parallelize the cross-validation for each run
mlmrepp <- matrix(unlist(future_lapply(1:runs, function(i) {
  mlmCV(X = X[[i]], Y = Y[[i]], k = fold)
})), ncol = p, byrow = TRUE)

# Calculate mean and standard deviation of cross-validation errors
lcv_mn <- colMeans(mlmrepp)
lcv_sd <- apply(mlmrepp, 2, sd)

# Save results to file
sink("LIRn30p5r3q2delta0_8_rho0_01.txt")
print(mlmrepp)
print(colMeans(mlmrepp))
print(colSds(mlmrepp))
sink()




#________________________________________________________________________________ 
#____________________________________Plots_______________________________________
#________________________________________________________________________________ 
# Load ggplot2 library
library(ggplot2)

# Create a data frame with the necessary values
df <- data.frame(
  x = rep(1:p, times = 4),  # x values for the different curves
  value = c(ecv_mn1, lcv_mn[1], scv_mn1, pcv_mn1),  # The values for each curve
  group = rep(c("EPLS", "SIMPLS", "EM-PLS", "OLS"), each = p)  # Different groups (models)
)

# Create the plot
ggplot(df, aes(x = x, y = value, color = group, shape = group, linetype = group)) +
  geom_line() +  # Plot the lines
  geom_point() +  # Plot points
  scale_color_manual(values = c("red", "blue", "black", "cyan3")) +  # Custom colors
  scale_linetype_manual(values = c(1, 2, 3, 4)) +  # Custom line types
  scale_shape_manual(values = c(20, 20, 20, 20)) +  # Shape of points
  ggtitle(expression(n == 30, p == 5, r == 3, q == 2, rho == 0.01)) +  # Title
  xlab("Number of components") +  # X-axis label
  ylab("Cross-validated RMSE") +  # Y-axis label
  theme_minimal() +  # Minimal theme
  theme(legend.position = "bottomright") +  # Move legend to bottom right
  geom_vline(xintercept = q, linetype = "dashed", color = "gray")  # Vertical line at q
